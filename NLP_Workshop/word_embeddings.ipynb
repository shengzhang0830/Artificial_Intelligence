{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class we learned about distributed representation of words which is also know as word embeddings. Neural network word embedding models generate word representation where the similarity between words could be directly interpreted. This type of vector representation could be used to predict the words that appear in a context of a given word. For a word $w_i$ we define a context $w_{c_i}$ which consists of the words that appear in a particular word window before and after it. Context words are modeled as a conditional distribution of the center word. \n",
    "\n",
    "$$ \\Large p(w_{c_i} | w_i) $$\n",
    "\n",
    "Word embeddings are in fact feature vectors which are assigned to each word in the collection. The conditional probability between a word $w_i$ and one of its context words $j: j \\in c_i$ is computed using the softmax function:\n",
    "\n",
    "$$ \\Large p(w_j | w_i)= \\frac{ \\exp ({v^{'}_j}^T v_i)}{ \\sum_{k=1}^V {\\exp ({v^{'}_k}^T v_i)}} $$\n",
    "\n",
    "Note that for each word we assign two types of features vectors $v^{'}$ and $v$ which are referred to as context and word embedding vectors. \n",
    "\n",
    "We use this conditional to define an objective which is the product of the conditionals of all the words in the collection. Word embeddings are obtained by optimizing the log of this product which is a sum log of the conditionals:\n",
    "\n",
    "$$ \\Large \\frac{1}{|C|} \\sum_{i=1}^{C} \\sum_{j=1}^{|c_i|} \\log p(w_j | w_i)$$\n",
    "\n",
    "Word embeddings are obtained by optimizing the objective using gradient based methods. \n",
    "\n",
    "In the class we learned about two word embeddings models - continuous bag of words (CBOW) and skip-gram. The major difference between the two models is how the conditional distribution is defined. In the skip-gram model each context word is conditioned on the observed word. This is the conditional distribution which we used at the beginning of this section. CBOW on the other hand models each observed word conditioned on its context:\n",
    "\n",
    "$$ \\Large p(w_i | w_{c_i}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab session we are going to obtain hands on experience in using word embeddings to represent words in a collection. More specifically we'll be working with the Gensim implementation of the word2vec family of word embedding models which consists of the skip-gram and CBOW models. We are going to be using the Amazon product reviews collection. To get a richer word embedding model we'll be using the version of the data that contains 10k reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Collection ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the Amazon product reviews data. Again, as a reminder the reviews data is semi-structured and is in a json file format. Below is a preview of this data which contains the entry for one review:  \n",
    "`\n",
    "{\n",
    "  \"reviewerID\": \"A3HVRXV0LVJN7\",\n",
    "  \"asin\": \"0110400550\",\n",
    "  \"reviewerName\": \"BiancaNicole\",\n",
    "  \"helpful\": [\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"reviewText\": \"Best phone case ever . Everywhere I go I get a ton of compliments on it. It was in perfect condition as well.\",\n",
    "  \"overall\": 5.0,\n",
    "  \"summary\": \"A++++\",\n",
    "  \"unixReviewTime\": 1358035200,\n",
    "  \"reviewTime\": \"01 13, 2013\"\n",
    "}\n",
    "`\n",
    "This dataset comes with a set of python functions that will help us convert the reviews from json format to Pandas dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these helper functions we'll extract the \"reviewText\" field from each review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "review_file = \"../../../data/amazon_reviews/cp/reviews_Cell_Phones_and_Accessories_h10k.json.gz\"\n",
    "\n",
    "df = getDF(review_file)\n",
    "print (df['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing ##\n",
    "Now that we've extracted the reviews we'll proceed by tokenizing them. In this next step we'll perform the following:  \n",
    "* Extract sentences\n",
    "* Tokenize words\n",
    "* Remove stopwords\n",
    "* Remove punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tree import Tree\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "#Create a list for the tokenized sentences:\n",
    "tok_sentences = list()\n",
    "#Create a translation table for removing the punctuation marks:\n",
    "translator=str.maketrans('','',string.punctuation)\n",
    "\n",
    "all_words = list()\n",
    "r_count=0\n",
    "for review  in df['reviewText']:\n",
    "    r_count+=1\n",
    "    if (r_count%1000==0):\n",
    "        print (r_count)\n",
    "    sentences = nltk.sent_tokenize(review)\n",
    "    for sentence in sentences:\n",
    "        sent_words = nltk.word_tokenize(sentence)\n",
    "        sent_words_tok = [word.lower() for word in sent_words if word.isalpha()]\n",
    "        tok_sentences.append(sent_words_tok)\n",
    "        for words in sent_words_tok:\n",
    "            all_words.append(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also obtain the sorted list of words based on their frequency count. This would help us get a sense better sense of the words present in this collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "frequency_count = nltk.FreqDist(all_words)\n",
    "words =np.array([word for word in frequency_count.keys()])\n",
    "word_freq=np.array([word for word in frequency_count.values()])\n",
    "freq_sort = np.argsort(word_freq)[::-1]\n",
    "word_freq_sort =word_freq[freq_sort]\n",
    "words_sorted = words[freq_sort]\n",
    "rank=1\n",
    "for object in words_sorted:\n",
    "    if (rank<=1000):\n",
    "        print(object+\"\\t\"+str(frequency_count[object]))\n",
    "    rank+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec ##\n",
    "With the Gensim package training a word embeddings model is straightforward. It requires a call to a single method named __Word2Vec__.During the lab session we'll learn about some of the input parameters of this method. The default settings of this method uses the CBOW model. This method expects as an input a list of sentences. In our case this list would be the output of the tokenized step above. Let's now use this method to train a word embedding model using the Amazon reviews data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "%env PYTHONHASHSEED=1 \n",
    "model = Word2Vec(tok_sentences, size=100, seed=1, window=5, min_count=1, iter=5)\n",
    "model.save(review_file+\".w2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use the trained model to explore the relationship between words in our collection. The __wv.most_similar__  method allows us to obtain the most similar words for a given input word. Below is an example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('quality')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 1]**  \n",
    "With the above method explore the word emeddings representation of the reviews collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Assignment 2]**\n",
    "Earlier in class we learned that the word2vec family of models capture semantic and syntactic relationships between the words. In particular we learned that using vector arithmetic we could capture word analogies such as the following:  \n",
    "“Man is to Women as King is to Queen”  \n",
    "which could be obtained by the following arithmentic operation:  \n",
    "“King” - ”Men” + “Women” ~ “Queen”\n",
    "The Gensim implementation of word2vec contains a method called __wv.most_similar()__ that for a given set of words returns a list of the most similar words that abide to this relationship. Let's look at several examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['sound', 'camera'], negative=['picture']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use this method to explore the generated word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 2]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 3]**\n",
    "the gensim implementation of the word2vec family of models provides additional methods that let's you further explore the generated embeddings space. Below are few methods along with their description and example use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Returns the word that is not related to the other words in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(\"samsung motorola lg iphone volume\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Computes the similarity between two words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('samsung', 'iphone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these methods to further explore the generated embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 3]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 4]**\n",
    "So far in our assignment we used the CBOW model. In this part of the lab session we are going to use the skip-gram model. You could train the skip-gram model by specifying the following parameter in the above __Word2Vec__ method:\n",
    "Word2Vec(tok_sentences, size=100, window=4, min_count=5, workers=4, sg=1, hs=1)\n",
    "Note that the default settings of the word2vec method uses negative embeddings while in this case we will be using the hiearchical softmax __hs=1__. \n",
    "Use some of the previous methods with the skip-gram model and see if you could observe a difference between the two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 4]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
