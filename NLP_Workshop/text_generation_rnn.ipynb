{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Recurrent Neural Networks\n",
    "\n",
    "In this notebook, we will see how to define and fit a recurrent neural network (RNN) using TensorFlow. We will fit the model on two ebooks, donwloaded in plain txt format from the [Project Gutenberg website](http://www.gutenberg.org/ebooks/). In particular, we will use \"Alice in Wonderland\" and \"Hamlet\".\n",
    "\n",
    "RNNs are computationally expensive to fit. Real-world applications are typically distributed or parallellized across multiple CPUs and GPUs. However, we only have a laptop with no GPU acceleration. Thus, we will make a set of simplifications:\n",
    "\n",
    "- We will train the RNN to predict *one character at a time*, rather than one word at a time. This has the advantage that we can reduce the input and output dimensionality (there are tens/hundreds thousands words versus around one hundred characters).\n",
    "\n",
    "- To further reduce the complexity, we will consider *lower case* characters only, i.e., we will convert all characters in the original text to lower case.\n",
    "\n",
    "- We will consider only reduced versions of the data (Chapters I, II, and III of \"Alice in Wonderland\"; and Act I of \"Hamlet\").\n",
    "\n",
    "Even with these considerations, fitting the model on a laptop still takes a few hours. So we will provide the fitted model results, so that you can directly load them. However, let's leave that for later, and let's focus now on how to leverage TensorFlow to fit a RNN for us.\n",
    "\n",
    "First, import the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First, load the (reduced) data for \"Alice in Wonderland\", and conver to lower case. Note that we will keep special characters, such as punctuation marks (`,` `.` `\"`) or carriage return (`\\n`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load (the reduced version of) \"Hamlet\" in plain text format\n",
    "filename = \"dat/hamlet-reduced.txt\"\n",
    "with open(filename) as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Use the cell below to convert the text in variable `raw_text` to lower case. (Overwrite the variable `raw_text`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Create the variable `n_chars_total` with the total number of characters. Print this number in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36502 characters in the text\n"
     ]
    }
   ],
   "source": [
    "n_chars_total = len(raw_text)\n",
    "print('There are {:d} characters in the text'.format(n_chars_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Use the cell below to print the first 1000 characters of the text and confirm that it is lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actus primus. scoena prima.\n",
      "\n",
      "enter barnardo and francisco two centinels.\n",
      "\n",
      "  barnardo. who's there?\n",
      "  fran. nay answer me: stand & vnfold\n",
      "your selfe\n",
      "\n",
      "   bar. long liue the king\n",
      "\n",
      "   fran. barnardo?\n",
      "  bar. he\n",
      "\n",
      "   fran. you come most carefully vpon your houre\n",
      "\n",
      "   bar. 'tis now strook twelue, get thee to bed francisco\n",
      "\n",
      "   fran. for this releefe much thankes: 'tis bitter cold,\n",
      "and i am sicke at heart\n",
      "\n",
      "   barn. haue you had quiet guard?\n",
      "  fran. not a mouse stirring\n",
      "\n",
      "   barn. well, goodnight. if you do meet horatio and\n",
      "marcellus, the riuals of my watch, bid them make hast.\n",
      "enter horatio and marcellus.\n",
      "\n",
      "  fran. i thinke i heare them. stand: who's there?\n",
      "  hor. friends to this ground\n",
      "\n",
      "   mar. and leige-men to the dane\n",
      "\n",
      "   fran. giue you good night\n",
      "\n",
      "   mar. o farwel honest soldier, who hath relieu'd you?\n",
      "  fra. barnardo ha's my place: giue you goodnight.\n",
      "\n",
      "exit fran.\n",
      "\n",
      "  mar. holla barnardo\n",
      "\n",
      "   bar. say, what is horatio there?\n",
      "  hor. a peece of him\n",
      "\n",
      "   bar. welcome horatio, welcome good marcellus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define now the \"vocabulary\" (i.e., the set of unique characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '&', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# List of unique chars in the text\n",
    "chars = sorted(list(set(raw_text)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Create the variable `vocab_size` with the number of unique characters. Print the number of unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "print(\"There are {} unique characters\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a dictionary that maps characters to integers (similarly to tokenization for words), and a reverse dictionary that maps integers to characters. We will use the reverse dictionary after fitting the model, at the text generation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "dictionary_char2int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Create the reverse dictionary `dictionary_int2char` (use the cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_int2char = dict(zip(dictionary_char2int.values(), dictionary_char2int.keys()))\n",
    "# Alternatively: \n",
    "# dictionary_int2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data as Sequences of Characters\n",
    "\n",
    "Now we will create data sequences based on the raw text. Although RNNs have the potential to keep track of long-term memory (specially GRUs and LSTMs), in practice that turns out to be expensive, so it is common to truncate the sequences to have some fixed, reduced length.\n",
    "\n",
    "In our case, we will use the variable `seq_length` and fit the RNN to predict the next character given a sequence of length `seq_length`. For instance, consider the phrase\n",
    "```\n",
    "alice was [...]\n",
    "```\n",
    "and consider a hypothetical example in which `seq_length=4`. Then, we would form the following sequences:\n",
    "```\n",
    "alic --> Predict \"e\"\n",
    "lice --> Predict \" \"\n",
    "ice  --> Predict \"w\"\n",
    "ce w --> Predict \"a\"\n",
    "e wa --> Predict \"s\"\n",
    " was --> Predict \" \"\n",
    "...\n",
    "```\n",
    "(Of course, we need a value greater than 4 for the sequence length; below we set `seq_length=100`.)\n",
    "\n",
    "We define the function below to create those sequences, together with the labels that need to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_sequences(seq_length):\n",
    "    # List of sequences\n",
    "    X_seq = [raw_text[i:i+seq_length] for i in range(n_chars_total-seq_length)]\n",
    "    # List of targets\n",
    "    Y_seq = [raw_text[i+seq_length] for i in range(n_chars_total-seq_length)]\n",
    "    # Return\n",
    "    return X_seq, Y_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we specify the value of `seq_length` and build the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "X_seq, Y_seq = create_sequences(seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Create variable `N` containing the total number of sequences. Print `N`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36402 sequences in total, each of length 100\n"
     ]
    }
   ],
   "source": [
    "N = len(X_seq)\n",
    "print(\"There are {} sequences in total, each of length {}\".format(N, seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** In the cell below, print one of the character sequences and its associated label to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQUENCE:\n",
      "scoena prima.\n",
      "\n",
      "enter barnardo and francisco two centinels.\n",
      "\n",
      "  barnardo. who's there?\n",
      "  fran. nay ans\n",
      "\n",
      "TARGET:\n",
      "w\n"
     ]
    }
   ],
   "source": [
    "print(\"SEQUENCE:\")\n",
    "print(X_seq[14])\n",
    "print(\"\")\n",
    "print(\"TARGET:\")\n",
    "print(Y_seq[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches of Data\n",
    "\n",
    "We cannot use all the `N` sequences at each iteration of the gradient descent algorithm; that would be computationally very expensive. In neural network applications, it is typically preferred to use stochastic gradient descent (SGD). In SGD, at each iteration of the algorithm we choose a subset of the training data and proceed as if this were the actual dataset.\n",
    "\n",
    "In the code below, we write the function `get_minibatch`, which at each iteration returns the next minibatch of `batch_size` sequences. It also converts the sequences to integer sequences (instead of char) according to the dictionary, and returns the results as numpy matrices.\n",
    "\n",
    "For simplicity, we don't choose the minibatch at random (as we should), but instead iterate over the sequences in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(N, batch_size):\n",
    "    for i in range(N // batch_size):\n",
    "        # Define the size of the numpy matrices\n",
    "        Xbatch = np.zeros((batch_size, seq_length)).astype(np.int32) # batch_size x seq_length\n",
    "        Ybatch = np.zeros((batch_size)).astype(np.int32)             # batch_size\n",
    "        # For each sequence in minibatch\n",
    "        for j in range(batch_size):\n",
    "            # Convert characters to int (for the sequences)\n",
    "            Xaux = [dictionary_char2int[c] for c in X_seq[i*batch_size+j]]\n",
    "            Xbatch[j,:] = Xaux\n",
    "            # Convert characters to int (for the targets)\n",
    "            Yaux = dictionary_char2int[Y_seq[i*batch_size+j]]\n",
    "            Ybatch[j] = Yaux\n",
    "        # Return (iterable object)\n",
    "        yield Xbatch, Ybatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders\n",
    "\n",
    "Now we declare placeholders for the data. (This is already part of TensorFlow's *computational graph*.) **Each placeholder will contain a minibatch of data**, and thus `X_minibatch` has size `batch_size x seq_length` and `Y_minibatch` has length `batch_size`. We will use a batch size of 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Declare the minibatch size\n",
    "batch_size = 30\n",
    "# Declare placeholders for minibatches of data\n",
    "X_minibatch = tf.placeholder(tf.int32, shape=(batch_size, seq_length))\n",
    "Y_minibatch = tf.placeholder(tf.int32, shape=(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to one-hot representation.** As input features for the RNN, we will use the one-hot representation of each character to avoid the need of learning feature vectors. In the cell below, we convert the sequences to their one-hot representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now convert the sequences to one-hot representation\n",
    "X_minibatch_one_hot = tf.one_hot(X_minibatch, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Use the cell below to find out the size (or shape) of `X_minibatch_one_hot`. What does each dimension of the tensor represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 100, 40)\n"
     ]
    }
   ],
   "source": [
    "print(X_minibatch_one_hot.shape)  # batch_size x seq_length x vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Recurrent Neural Network\n",
    "\n",
    "**1. Define the neural network**\n",
    "\n",
    "Now we build the neural network. TensorFlow has built-in support for standard RNNs, as well as GRUs and LSTMs. We will use GRUs because they can capture long-term dependencies better than standard RNNs.\n",
    "\n",
    "RNNs have two parameters: the number of layers (`num_layers`) and the number of hidden units in each layer (`hidden_units`). In the cell below, we create a two-layer GRU with 256 hidden units per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "num_layers = 2\n",
    "# Number of hidden units per layer\n",
    "hidden_units = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Form a list of GRU cells (one per layer)\n",
    "cell_list = []\n",
    "for h in range(num_layers):\n",
    "    cell_list.append(tf.contrib.rnn.GRUCell(hidden_units)) # h-th layer (GRU cell)\n",
    "# Stack all GRU cells in the list\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cell_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. Apply the NN to the data**\n",
    "\n",
    "We now apply the 2-layer GRU to all the sequences in the minibatch. Specifically, this means to compute all the inner products and non-linearities, up to the end of each sequence. TensorFlow's function `dynamic_rnn` does that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply the RNN to an entire minibatch of data\n",
    "output, _ = tf.nn.dynamic_rnn(cell, X_minibatch_one_hot, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Use the cell below to print the size of the variable `output`. What do these numbers correspond to? Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 100, 256)\n"
     ]
    }
   ],
   "source": [
    "print(output.shape)   # batch_size x seq_lenth x hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Predict the next character**\n",
    "\n",
    "Now we want to predict the next character, based on the output of the RNN. Recall that, for each sequence in the minibatch, the output is a `seq_length x hidden_units` tensor. We will not use the outputs for the intermediate time steps in the sequence, but only the last one, which is a vector of length `hidden_units` (for each sequence). The line below gathers the output for the last time step and reshapes it as a `batch_size x hidden_units` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep only the output for the last time step\n",
    "output_last = tf.squeeze(output[:, seq_length-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Print the shape of `output_last` and check whether it has the expected size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 256)\n"
     ]
    }
   ],
   "source": [
    "print(output_last.shape)   # batch_size x hidden_units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the output from the last time step to predict the next character. For that, we will include a softmax layer, which converts the output dimension (`hidden_units`) to the logits (of length `vocab_size`). Recall that a softmax layer has weights and intercepts.\n",
    "\n",
    "**[Task]** Declare the parameters for the softmax layer as TensorFlow variables. Use the name `weights_softmax` for the weights and `intercept_softmax` for the intercepts. Be careful to choose the appropriate size. Initialize the intercept to zero, and initialize the weights randomly following a truncated normal distribution (`tf.truncated_normal`) with standard deviation $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Declare and initialize the softmax parameters: weights and intercepts\n",
    "weights_softmax = tf.Variable( tf.truncated_normal((hidden_units, vocab_size), stddev=0.01) )\n",
    "intercept_softmax = tf.Variable( tf.zeros(vocab_size), dtype=tf.float32 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute the logits using these variables.\n",
    "\n",
    "**[Task]** In the cell below, compute the variable `logits`. Print the size of `logits` to check that it has the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 40)\n"
     ]
    }
   ],
   "source": [
    "# Compute the logits as usual (weight*output_of_previous_layer + intercept)\n",
    "logits = tf.matmul(output_last, weights_softmax) + intercept_softmax\n",
    "print(logits.shape)  # batch_size x vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Compute the loss**\n",
    "\n",
    "Now we compute the loss, which is the standard average log-likelihood of the predicted labels, similarly to multinomial logistic regression:\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{B} \\sum_{b=1}^B \\log (\\widehat{y}_b),\n",
    "$$\n",
    "where $b$ denotes the minibatch, and $\\widehat{y}_b$ denotes the predicted probability for the observed class (label).\n",
    "\n",
    "TensorFlow hast the function `sparse_softmax_cross_entropy_with_logits`, which computes that for us, up to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the loss for each sequence in the minibatch\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=Y_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Compute the average of the loss into variable `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the average\n",
    "loss = tf.reduce_mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Define the optimizer**\n",
    "\n",
    "As in most TensorFlow applications, we ultimately want to solve an optimization problem. In this case, we want to minimize the loss. For that, we use Adadelta optimizer (we could also use other), with learning rate 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 1.0\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Optimization: TensorFlow Session\n",
    "\n",
    "Here, we will create a session to optimize the model parameters. But first, we use a `Saver` object to save the model parameters once they have been fit. The reason is that we want to reuse the model parameters to make predictions, but fitting the model is computationally expensive, so we want to fit the model first, and then save the parameters to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Object to allow saving/restoring the value of TensorFlow variables within a session\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run 100 epochs of the optimization algorithm.\n",
    "\n",
    "**[Task]** Run the cell below. Wait for some time. Then interrupt the kernel (from the menu above, choose Kernel --> Interrupt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of passes over the data\n",
    "num_epochs = 100\n",
    "# Open a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    # For each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        n_iter = 0\n",
    "        print('Epoch ' + str(epoch) + '...')\n",
    "        # For each minibatch in the dataset\n",
    "        for X, Y in get_minibatch(N, batch_size):\n",
    "            # Feed the placeholder variables\n",
    "            feed_dict = {X_minibatch:  X,\n",
    "                         Y_minibatch:  Y}\n",
    "            # Run a gradient descent step and evaluate the loss\n",
    "            _, numpy_loss = sess.run([optimizer, loss], feed_dict=feed_dict)\n",
    "            # Print progress to screen\n",
    "            if(n_iter%100==0):\n",
    "                print('   iter ' + str(n_iter) + ': loss=' + str(numpy_loss))\n",
    "            n_iter += 1\n",
    "    # After convergence, save the session to disk\n",
    "    save_path = saver.save(sess, \"trained_models/trained_model_hamlet_h2x256_seq100.ckpt\")\n",
    "    print('Model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the code above runs slowly on a laptop. It may take hours to converge (you may leave the computer running overnight). For that reason, **we will provide you with the files that are saved after convergence**.\n",
    "\n",
    "## Text Generation\n",
    "\n",
    "**1. Preparing the test sequence**\n",
    "\n",
    "Below, we use an example of a short sentence that we will feed to the fitted RNN. You may replace the text with anything you like; just keep in mind that you *must* use lower case characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 44, 40)\n"
     ]
    }
   ],
   "source": [
    "# Initial string (you may replace with yours)\n",
    "test_string = 'King. We doubt it nothing, heartily farewell'.lower()\n",
    "# Number of new characters to generate\n",
    "n_new_characters = 500\n",
    "# Number of time steps of the initial string\n",
    "time_test = len(test_string)\n",
    "# Convert the characters to int\n",
    "X_test = [dictionary_char2int[c] for c in list(test_string)]\n",
    "# Reshape as a numpy matrix\n",
    "X_test = np.reshape(X_test, (1, time_test)).astype(np.int32)\n",
    "\n",
    "# Declare a placeholder for the test sequence\n",
    "Xtest_placeholder = tf.placeholder(tf.int32, (1, time_test))\n",
    "# Convert the placeholder variable to one-hot representation\n",
    "Xtest_one_hot = tf.one_hot(Xtest_placeholder, vocab_size)\n",
    "# Print its size as a sanity check\n",
    "print(Xtest_one_hot.shape)  # 1 x len(test_string) x vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Running the RNN**\n",
    "\n",
    "We will generate text as follows. We will run the RNN through the test sequence, `test_string`. We will pass the output through a softmax layer to obtain the probabilities of the next character. Then we will sample one character according to the softmax probabilities. We will use this new character as a new input for the RNN, generating another character, and so on. The pseudocode would look like this:\n",
    "\n",
    "```\n",
    "1. output, state = apply_rnn(test_string)\n",
    "2. sampled_char ~ softmax(output * weights + intercept)\n",
    "3. output, state = apply_rnn(sampled_char, initial_state=state)\n",
    "4. Go to 2.\n",
    "```\n",
    "\n",
    "The following code does that.\n",
    "\n",
    "**[Warning]** It may take 2-10 minutes to create the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reuse the same RNN as above\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "# 1. Apply the RNN to the test string\n",
    "test_output, test_state = tf.nn.dynamic_rnn(cell, Xtest_one_hot, dtype=tf.float32)\n",
    "test_output_last = tf.squeeze(test_output[:,time_test-1,:])\n",
    "test_output_last = tf.expand_dims(test_output_last, 0)\n",
    "# 2. Compute the softmax logits and sample a new character\n",
    "test_logits = tf.matmul(test_output_last, weights_softmax) + intercept_softmax\n",
    "sampled_char = tf.multinomial(test_logits, 1)\n",
    "# Convert the sampled character to one-hot representation\n",
    "sampled_value = tf.one_hot(sampled_char, vocab_size)\n",
    "# For each new character to be sampled:\n",
    "char_list = []\n",
    "for t in range(n_new_characters):\n",
    "    # 3. Run 1 step of the RNN\n",
    "    test_output, test_state = tf.nn.dynamic_rnn(cell, sampled_value, initial_state=test_state, dtype=tf.float32)\n",
    "    test_output_reshaped = tf.reshape(test_output, (1, hidden_units))\n",
    "    # 4. Compute the logits and sample a new character\n",
    "    test_logits = tf.matmul(test_output_reshaped, weights_softmax) + intercept_softmax\n",
    "    sampled_char = tf.multinomial(tf.expand_dims(test_logits[0,:], 0), 1)\n",
    "    # Convert the sampled character to one-hot representation\n",
    "    sampled_value = tf.one_hot(sampled_char, vocab_size)\n",
    "    # Append the sampled character to the list of characters\n",
    "    char_list.append(tf.squeeze(sampled_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Running a session**\n",
    "\n",
    "Now, we create a TensorFlow session to recover `char_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained_model_hamlet_h2x256_seq100.ckpt\n"
     ]
    }
   ],
   "source": [
    "# In a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    # Load the model\n",
    "    saver.restore(sess, 'trained_models/trained_model_hamlet_h2x256_seq100.ckpt')\n",
    "    # Evaluate char_list (the predicted list of characters)\n",
    "    char_list_numpy = sess.run([char_list], feed_dict={Xtest_placeholder: X_test})\n",
    "    \n",
    "# Convert char_list_numpy to a list of characters\n",
    "char_list_str = [dictionary_int2char[c] for c in char_list_numpy[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Use the cell below to print the test string, followed by the list of predicted characters. You may use `join` to concatenate the characters in the list into a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king. we doubt it nothing, heartily farewell\n",
      "\n",
      "exit voltemand and cornelius.\n",
      "\n",
      "and now laertes, hamlous drease stand\n",
      "\n",
      "   aarnllls, and he houle and for in,\n",
      "the blayder sire dis porpelyus this wirlin,\n",
      "the hands speake of this that you haue heard:\n",
      "sweare by my sword\n",
      "\n",
      "   gho. sweare\n",
      "\n",
      "   ham. well said old mole, can'st worke i'th' ground so fast?\n",
      "a worthy pioner, once more remoue good friends\n",
      "\n",
      "   hor. oh day and night: but this is wondrous strange\n",
      "\n",
      "   ham. and therefore as a stranger giue it welcome.\n",
      "there are more things in heauen and earth, h\n"
     ]
    }
   ],
   "source": [
    "# Print the test string, followed by the list of predicted charaters\n",
    "print(test_string + ''.join(char_list_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alice in Wonderland\n",
    "\n",
    "**[Task]** Repeat the process above, but using \"Alice in Wonderland\" instead of \"Hamlet\". You weel need to restart the python kernel before that (menu Kernel --> Restart & Clear output). You will need to change:\n",
    "- The name of the data file to be loaded.\n",
    "- The test sequence.\n",
    "- The name of the model to be restored (and maybe the name of the model to be saved, if you plan to leave it running overnight)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As you can see, the generated text recovers some of the most common words in the text, and it looks like English. However, is it still far from being perfect, as it may produce words that are not proper English words, and it may also overfit to the data. Here are some suggestions that can help improve its quality:\n",
    "- Use more data instead of the reduced text. We could go even further, and fit a RNN to all Shakespeare's plays.\n",
    "- Apply regularization techniques, such as dropout.\n",
    "- Use a more complex neural network. We are using a GRU with 2 layers and 256 hidden units, but that may not be enough.\n",
    "- Explore several initializations and/or values for the batch size and the stepsize schedule, which may have an impact on the optimum."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
