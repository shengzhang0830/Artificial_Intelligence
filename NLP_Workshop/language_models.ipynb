{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Language Models #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language models are one of the most important concepts in NLP. They help us assign probabilities to sequence of words, such as sentences or phrases. As we saw in the previous lecture their application spans beyond NLP. For example, language models help us obtain better speech recognition, optical character recognition (OCR) and information retrieval results, to name a few. Given a sentence $S$ with a set of $w_i$ words, $i=1,2,...,n$, language models formally define the probability of the sentence as the probability of having the particular sequence of words:  \n",
    "$$ \\Large p(S)=p(w_1, w_2, w_3, ... , w_n) $$  \n",
    "\n",
    "They also help us predict the probability of a specific word given the previous words in the sentence:  \n",
    "\n",
    "$$ \\Large p(w | w_{-1}, w_{-2}..,w_{-k}) $$  \n",
    "\n",
    "The probability of a sentence is computed using the chain rule:  \n",
    "\n",
    "$$ \\Large p(S)=p(w_1, w_2, w_3, ... , w_n) = \\prod_i^n {p(w_i|w_1,w_2,...,w_{i-1})} $$  \n",
    "\n",
    "When computing this probability we use the Markov assumption which simplifies the computation of the above probability:    \n",
    "\n",
    "$$ \\Large p(S)=p(w_1, w_2, w_3, ... , w_n) = \\prod_i^n {p(w_i|w_{i-k},w_{i-(k-1)},...,w_{i-1})} $$  \n",
    "\n",
    "The Markov assumption states that the conditional probability of a word $w_i$, given all of the previous words in the sentence, could be approximated by considering only the $k$ previous words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Models ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram Model ####\n",
    "The unigram language model is the simplest of the n-gram models. Under this model the probability of the sentence $S$ is simply a product of the probabilities of each individual word in the sentence:  \n",
    "\n",
    "$$ \\Large p(S) = \\prod_i^n {p(w_i)} $$  \n",
    "\n",
    "N-gram probabilities are computed using Maximum Likelihood Estimates (MLE). For the unigram language model we compute MLE by counting the number of times word $w_i$ occurred in the collection and we divide that number by the total number of words $v$ in the collection:\n",
    "$$ \\Large p(w_i) = \\frac{count(w_i)}{\\sum_{i=1}^{v}{count(w_i)}} $$  \n",
    "\n",
    "#### Bigram Model ####\n",
    "In the bigram model the probability of a word is conditioned only on the previous word:  \n",
    "$$ \\Large p(S) = \\prod_i^n {p(w_i|w_{i-1})} $$  \n",
    "\n",
    "The MLE for the bigram LM is computed by dividing the number of times words $w_i$ and $w_{i-1}$ occurred together in the collection with the number of occurrences of word $w_i$, $count(w_{i-1})$ :\n",
    "\n",
    "$$ \\Large p(w_i|w_{i-1}) = \\frac{count(w_{i-1},w_i)}{count(w_{i-1})} $$  \n",
    "\n",
    "#### Trigram Model ####\n",
    "In the trigram model the probability of a word is conditioned on the previous two words:  \n",
    "$$ \\Large p(S) = \\prod_i^n {p(w_i|w_{i-2},w_{i-1})} $$  \n",
    "\n",
    "The MLE for the trigram LM is computed by dividing the number of times words $w_i$, $w_{i-1}$, $w_{i-2}$ occurred together in the collection with the count for $w_{i-1}$ and $w_{i-2}$ occurring together, $count(w_{i-1},w_{i-2})$:\n",
    "\n",
    "$$ \\Large p(w_i|w_{i-1},w_{i-2}) = \\frac{count(w_{i-2},w_{i-1},w_i)}{count(w_{i-2},w_{i-1})} $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Bigrams ##\n",
    "In this task we are going to compute unigram and bigram language models over a collection of news stories from various news agencies that were published by the Associated Press Worldstream English Service on September 30th 2010. Rather than implementing language models on our own we are going to use the nltk package. Together let's go over the code step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first load the NLTK stopwords list and also define an array of punctuation marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import itertools\n",
    "#load the NLTK stopwords list:\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to read the collection of news stories. This collection is stored in a tsv file where each news story is stored into a single line. In each line the number of tsv fields varies but the first three fields always contain the same type of an information. These fields are:  \n",
    "__article_id__, __location with date and time__ and the __news story title__.  \n",
    "Here is an example:  \n",
    "APW_ENG_20100930.0015__\\t__TOKYO 2010-09-30 00:10:50 UTC__\\t__Japan factory output down for third straight month  \n",
    "The remaining tab separated fields within the line contain the news story sentences.  \n",
    "One way to load this data would be through pandas but due to the variable number of tsv fields across lines we wouldn't be able to do that. Therefore we'll use the traditional approach of reading a file line by line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsstories = \"../../../data/gigaword_20b_1k\"\n",
    "\n",
    "articles = open(newsstories,'r')\n",
    "all_words = list()\n",
    "a_count=0\n",
    "#Create a translation table:\n",
    "translator=str.maketrans('','',string.punctuation)\n",
    "\n",
    "tokenized_sentences = list()\n",
    "for article in articles.readlines():\n",
    "    article = article.strip()\n",
    "    a_count += 1\n",
    "    if (a_count % 100 == 0):\n",
    "        print(a_count)\n",
    "    fields = article.split(\"\\t\")\n",
    "    article_id = fields[0]\n",
    "    place = fields[1].split(\" \")[0]\n",
    "    date = fields[1].split(\" \")[1]\n",
    "    time = fields[1].split(\" \")[2]\n",
    "    title = fields[2]\n",
    "    \n",
    "    article_content = fields[2::]\n",
    "    for sent in article_content:\n",
    "        tokenized_sent = list()\n",
    "        sent_words = nltk.word_tokenize(sent)\n",
    "        sent_words = [word for word in sent_words if ((len(word) > 1) and (len(word) < 20))]\n",
    "        sent_words = [word for word in sent_words if word not in stopwords_list]\n",
    "        for word in sent_words:\n",
    "            word = word.lower()\n",
    "            word=word.translate(translator)\n",
    "            all_words.append(word)\n",
    "            tokenized_sent.append(word)\n",
    "        tokenized_sentences.append(tokenized_sent)\n",
    "        print(tokenized_sent)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will compute the bigram probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First compute the bigrams (i.e. the tuple of words that occur together)\n",
    "bigram_model  = nltk.bigrams(all_words)\n",
    "\n",
    "#Then compute the frequency for each bigram\n",
    "bigram_frequency_word = nltk.ConditionalFreqDist(bigram_model)\n",
    "#To get the bigram probabilities we would need to normalize the bigram frequencies:\n",
    "bigram_probability_word = nltk.ConditionalProbDist(bigram_frequency_word, nltk.MLEProbDist)\n",
    "\n",
    "#Let's compute the bigram probabilities constraining on sentences:\n",
    "bigram_frequency_sent = nltk.ConditionalFreqDist((word[0],word[1]) for word in list( itertools.chain (*[nltk.bigrams(sent) for sent in tokenized_sentences])))\n",
    "bigram_probability_sent = nltk.ConditionalProbDist(bigram_frequency_sent, nltk.MLEProbDist)\n",
    "\n",
    "#Let's print the bigram probabilities computed over sentences:\n",
    "bigram_probability = bigram_probability_sent\n",
    "bigram_frequency = bigram_frequency_sent\n",
    "\n",
    "all_bigrams = dict()\n",
    "for source_word in bigram_probability:\n",
    "\tprob_words = bigram_probability[source_word].samples()\n",
    "\tdenom = len(prob_words)\n",
    "\tall_bigrams[source_word]=dict()\n",
    "\tfor target_word in prob_words:\n",
    "\t\tprob = bigram_probability[source_word].prob(target_word)\n",
    "\t\tall_bigrams[source_word][target_word] = prob\n",
    "\t\tprint (\"p(\"+target_word+\"|\"+source_word+\")={0:.4f}\".format(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated the bigram language model we could make query about any bigram and obtain its probability using the following code:  \n",
    "bigram_probability[$w_{i-1}$].prob($w_{i}$)  \n",
    "Where $w_{i-1}$ is the prior word and $w_{i}$ is the current word. For example the code below would give us the probability of the bigram \"fiat ceo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (bigram_probability[\"fiat\"].prob(\"ceo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given word in our collection we could also obtain the probability of all of its bigrams. For example the code below gives us all the possible bigrams for the word \"september\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_bigrams[\"september\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 1]**  \n",
    "Use the above code to explore the bigram model that we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Unigrams##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our lab session on Tuesday we used the __nltk.FreqDist__ method to obtain the frequency count over all the words in our collection. The input argument to this method is the list of words in the collection. This method returns a dictionary where the keys are the words in the collection and the values are their frequency counts. For example if the word __car__ occurred 10 times in our collection the dictionary value for this word would be 10: __dict[\"car\"]=10__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 2]**  \n",
    "In this part of the lab session you are asked to compute the unigram probabilities using the __nltk.FreqDist__ method. At the beginning of this lab session we stored all words in the collection in the __all_words__ list. To compute the unigram probability for each word in the collection you would need to divide the frequency count of that word with the total number of word instances in the collection which is equal to the sum of all frequency counts. \n",
    "Provided below is the code framework that you could use for this task. In order to compute the unigram probabilities you would need to implement the missing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_frequency = nltk.FreqDist(all_words)\n",
    "#2a: Compute the total number of word instances in the collection. This is the sum of all word frequencies. \n",
    "sum_freq = ?\n",
    "#Use the following dictionary to store the unigram probabilities for each word:\n",
    "all_unigrams = dict()\n",
    "#In order to compute the unigram probability for each word you would need to \n",
    "#iterate over all the words in the collection:\n",
    "for word in unigram_frequency:\n",
    "    #2b: Obtain the word frequency count:\n",
    "    freq = ?\n",
    "    #2c: Compute the probability:\n",
    "    prob = (1.0)*freq/?\n",
    "    #Store the probability value into our dictionary. \n",
    "    all_unigrams[word]= prob\n",
    "    print (\"p(\"+word+\")={0:.4f}\".format(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Assignment 3]**  \n",
    "Now that we've computed the unigram probabilities you could obtain the probability for each word in the collection by querying the __all_unigrams__ dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
