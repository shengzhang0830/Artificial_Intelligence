{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models - Latent Dirichlet Allocation (LDA) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous class we covered Latent Dirichlet Allocation (LDA) which is the most widely used topic model. We also discussed some of the LDA extensions such as the polylingual topic model (PLTM). For a collection of D documents with a collection wide vocabulary of V words, the model assumes that its only observable variable are the actual words in the document and that the number of topics in the collection is a priori set to T. For each topic $t=1,2,3,...,T$ in the collection, the model first draws a $V$ dimensional multinomial distribution $beta_t$ from a prior Dirichlet distribution with hyperparameter $\\eta: \\beta_t \\sim Dir(\\eta)$.  For each document $d=1,2,3,...,D$ in the collection, LDA's generative process includes the following steps:\n",
    "* Draw a multinomial distribution $\\theta_d$ from a collection wide Dirichlet distribution with hyperparameter $\\alpha: \\theta_d \\sim Dir(\\alpha)$.\n",
    "* Go over each word position $n=1,2,3,..., N_d$ in document d and assign a topic indicator $z_n$ by drawing topic from  $\\theta_d: z_n \\sim Multinomial(\\theta_d)$.\n",
    "* Based on the drawn topic assignment $z_n=t$, draw the actual word $w_n$ from the topic specific distribution over words: $w_n \\sim Multinomial(\\theta_d)$.  \n",
    "The above process is then repeated for every document in the collection. In LDA we deal with two Dirichlet distributions which are used as prior distributions for the document-topic distribution $\\theta_d$ and the topic-word distributions $\\beta_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab session we are going to use LDA as a topic model to infer topics over documents and represent documents in a shared topic space. We'll working with the Gensim implementation of LDA. In terms of the dataset we will be using the Amazon products review collection. The gensim implimentation of LDA uses online variational inference to infer the posterior distributions. More about this approach could be found in the following paper:  \n",
    "* M. Hoffman, D. Blei, and F. Bach. Online learning for latent Dirichlet allocation. Neural Information Processing Systems, 2010.  \n",
    "http://www.cs.columbia.edu/~blei/papers/HoffmanBleiBach2010b.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Collection ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the Amazon product reviews data. Again, as a reminder the reviews data is semi-structured and is in a json file format. Below is a preview of this data which contains the entry for one review:  \n",
    "`\n",
    "{\n",
    "  \"reviewerID\": \"A3HVRXV0LVJN7\",\n",
    "  \"asin\": \"0110400550\",\n",
    "  \"reviewerName\": \"BiancaNicole\",\n",
    "  \"helpful\": [\n",
    "    4,\n",
    "    4\n",
    "  ],\n",
    "  \"reviewText\": \"Best phone case ever . Everywhere I go I get a ton of compliments on it. It was in perfect condition as well.\",\n",
    "  \"overall\": 5.0,\n",
    "  \"summary\": \"A++++\",\n",
    "  \"unixReviewTime\": 1358035200,\n",
    "  \"reviewTime\": \"01 13, 2013\"\n",
    "}\n",
    "`\n",
    "This dataset comes with a set of python functions that will help us convert the reviews from json format to Pandas dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these helper functions we'll extract the \"reviewText\" field from each review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "review_file = \"../../../data/amazon_reviews/cp/reviews_Cell_Phones_and_Accessories_h10k.json.gz\"\n",
    "\n",
    "df = getDF(review_file)\n",
    "print (df['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing ##\n",
    "Now that we've extracted the reviews we'll proceed by tokenizing them. In this next step we'll perform the following:  \n",
    "* Extract sentences\n",
    "* Tokenize words\n",
    "* Remove stopwords\n",
    "* Remove punctuation marks\n",
    "\n",
    "Note how we now focus on the reviews rather than the sentences. Reason being is that LDA doesn't operate well on sentence level due to the small number of words that could typically be present in any sentence which would not give us sufficient statistics to infer meaningful topic distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tree import Tree\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')\n",
    "# Create a list for the tokenized sentences:\n",
    "tok_sentences = list()\n",
    "# Create a list for the tokenized reviews:\n",
    "tok_reviews = list()\n",
    "# Create a list for the sentence assigned POS tags:\n",
    "pos_sentences = list()\n",
    "# Create a translation table for removing the punctuation marks:\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "all_words = list()\n",
    "r_count = 0\n",
    "for review in df['reviewText']:\n",
    "    r_count += 1\n",
    "    if (r_count % 1000 == 0):\n",
    "        print(r_count)\n",
    "    sentences = nltk.sent_tokenize(review)\n",
    "    review_words= list()\n",
    "    for sentence in sentences:\n",
    "        sent_words = nltk.word_tokenize(sentence)\n",
    "        sent_words_tok = [word.lower() for word in sent_words if word not in stopwords_list and word.isalpha()]\n",
    "        tok_sentences.append(sent_words_tok)\n",
    "        for words in sent_words_tok:\n",
    "            all_words.append(words)\n",
    "            review_words.append(words)\n",
    "    tok_reviews.append(review_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models use the bag of words representation of documents. When inferring topics they rely on sufficient statistics over the number of times words appear in a document. To that end it is typically the case that rare words are removed from the collection. This is definitely the case with hapax words but in many instances, depending on the vocabulary size, words whose frequency of appearance is less than 10 times are also removed. For more details on how to preprocess collections for topic modeling you should refer to the following book chapter which is a very useful resource:\n",
    "* Boyd-Graber, J., Mimno, D., & Newman, D. (2014). Care and feeding of topic models: Problems, diagnostics, and improvements. Handbook of mixed membership models and their applications, 225255 (2014).  \n",
    "https://mimno.infosci.cornell.edu/papers/2014_book_chapter_care_and_feeding.pdf  \n",
    "\n",
    "In the next step we'll also obtain the frequency count of words which would help us generate the effective vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "frequency_count = nltk.FreqDist(all_words)\n",
    "words =np.array([word for word in frequency_count.keys()])\n",
    "word_freq=np.array([word for word in frequency_count.values()])\n",
    "freq_sort = np.argsort(word_freq)[::-1]\n",
    "word_freq_sort =word_freq[freq_sort]\n",
    "words_sorted = words[freq_sort]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effective vocabulary consists of the words that we will choose to use to represent the collection. In this case we'll discard hapax words (i.e. words whose frequency of occurrence is 1). We will also treat the top 25 words in the collection as stop words. These words will not be included in our effective vocabulary. Let's create our effective vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank=1\n",
    "effective_vocab=list()\n",
    "for object in words_sorted:\n",
    "    if (rank>=25):\n",
    "        fc = frequency_count[object]\n",
    "        if (fc>1):\n",
    "            effective_vocab.append(object)\n",
    "    rank+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created our effective vocabulary we'll go back and represent our set of reviews using this vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_reviews_ev = list()\n",
    "for review in tok_reviews:\n",
    "    review_words_ev = [word for word in review if word in effective_vocab]\n",
    "    tok_reviews_ev.append(review_words_ev)\n",
    "print(tok_reviews_ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model Training ##\n",
    "Next we'll train a LDA model. We do that using the __gensim.models.LdaModel__ method. Before proceeding with training the model we will create a bag of words representation of our collection using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(tok_reviews_ev)\n",
    "dictionary.save(\"../../../data/amazon_reviews/cp/reviews_Cell_Phones_and_Accessories_h1k.dict\")\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tok_reviews_ev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaModel(corpus,id2word=dictionary, num_topics=30, iterations=2000, alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 1]**  \n",
    "the gensim lda implementation contains various useful methods to explore the inferred embedding space. For example, the methods __.model.print_topics(num_topics=, num_words=)__ lets us obtain the top __num_words__ for each topic of the __num_topics__ . Use this method to explore the inferred set of topic-word distributions. More about the various lda methods including their description could be found here:  \n",
    "https://radimrehurek.com/gensim/models/ldamodel.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Solution 1]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 2]**\n",
    "Look into the LDA model parameters such as the number of iterations over the collection (__iterations__), number of topics (__num_topics__), the __alpha__ and __rho__ hyperparameters. Train the LDA model with different parameter configurations and observe how they affect the obtained topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 2]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Assignment 3]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once we train a LDA model we could use it to infer topics onto unseen documents. We do this with the call to the following method:  \n",
    "review_topics = lda[__new_doc_content__]  \n",
    "As in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_review = ['very', 'cheap', 'exactly', 'supposed', 'traveled', 'hands', 'free', 'problem', 'more', 'problem','yet', 'feels', 'durable']\n",
    "new_review_bow = dictionary.doc2bow(new_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a new review using words from the effective vocabulary and utilize this method to infer topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 3]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also plot the document-topic, i.e. the review-topic distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "topic_vec = np.zeros(30)\n",
    "for k, prob in new_review_topics:\n",
    "    topic_vec[k] = prob\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = (8,4)\n",
    "fig = plt.figure()\n",
    "x_pos = np.arange(len(topic_vec))\n",
    "plt.bar(x_pos,topic_vec)\n",
    "plt.ylabel('Probability(Topic)')\n",
    "plt.xlabel('Topic #')\n",
    "plt.title('Per Review Topic Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 4]**  \n",
    "For a given topic you could use the same approach to plot the per topic-word distribution. Choose a topic and analyze its topic distribution over its words. Note that the numbers on the x-axis will correspond to the indices assigned to the words. To obtain the ten most probable words for a topic you could use the __show_topic(topicid, topn=)__ where the topn input argument let's you choose the top n most probable words that will be retrieved for that given topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = lda.show_topic(10, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list()\n",
    "probs = list()\n",
    "for word, prob in top_words:\n",
    "    words.append(word)\n",
    "    probs.append(prob)\n",
    "x_pos = np.arange(len(probs))\n",
    "plt.bar(x_pos, probs, align='center', alpha=0.5)\n",
    "plt.xticks(x_pos, words)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Programming language usage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a certain topic and use the code above to analyze its probability distribution across the topn words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 4]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 5]**\n",
    "Explore the topic space on your own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 5]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
