{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "Many tasks in information retrieval (IR) and natural language processing (NLP) involve performing document similarity comparisons. These tasks include document clustering, retrieving the most relevant documents for a given query, finding document translation pairs in a large multilingual collection, etc.  \n",
    "\n",
    "Most practical applications of document similarity represent documents in a common vector space that captures document features. Representing documents in a feature space abstracts away from the specific sequence of words used in each document and, with appropriate representations, can also facilitate the analysis of relationships between documents written using different vocabularies.\n",
    "\n",
    "Here, we will cover one of the fundamental retrieval models using the tf-idf representation of documents. Queries and documents are represented in a space whose dimensions are the vocabulary terms (words, n-grams, stems, phrases, etc.). More specifically, for a document $d$ and a vocabulary $V$ of index terms this representation is a vector whose dimensions are the tf-idf weights for each of the index terms.\n",
    "\n",
    "The tf-idf representation of documents is typically sparse, because documents only contain a subset of the words in the vocabulary. This allows for the whole document collection to be represeted as a sparse matrix, where the rows are the documents and the columns correspond to the indexed terms.\n",
    "\n",
    "Here we will use the following form of the tf-idf representation:\n",
    "$$\n",
    "\\textrm{weight}_{wd} = \\frac{f_{wd}}{\\sum_{w^\\prime} f_{w^\\prime d}} \\times \\log\\left(\\frac{\\textrm{number of documents}}{df_w}\\right).\n",
    "$$\n",
    "\n",
    "Let's first import the packages that we will use in this session. In particular, we will use `TfidfVectorizer` from scikit-learn, which allows us to obtain the tf-idf representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import sklearn.metrics.pairwise\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Let's now read the data. We will use a small subset of books extracted from the [Project Gutenberg website](http://www.gutenberg.org/ebooks/).\n",
    "\n",
    "By default, the code below reads the books under folder `./gutenberg_subset`. Don't forget to replace the path with the appropriate path in your computer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath = './gutenberg_subset'  # <-- Replace with the appropriate path\n",
    "books = {}\n",
    "book_names = []\n",
    "exclude = set(string.punctuation)\n",
    "for filename in sorted(os.listdir(datapath)):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        try:\n",
    "            # Open the text and convert to lower case\n",
    "            book = open(os.path.join(datapath, filename)).read().lower()\n",
    "            # Remove punctuation\n",
    "            book = ''.join(ch for ch in book if ch not in exclude)\n",
    "            # Keep the book in a dictionary indexed by its filename\n",
    "            books[filename] = book\n",
    "            book_names.append(filename)\n",
    "        except:\n",
    "            # If error, do nothing\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Find out the number of books in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('There are {} books'.format(len(books)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Print the first 2000 characters of the book whose filename is `12381.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(books['12381.txt'][0:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute tf-idf representation.** For that, we use scikit-learn's `TfidfVectorizer`. This allows to automatically remove English stopwords.\n",
    "\n",
    "Note that the method `fit_transform` gives the tf-idf representation of a list of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the collection of books into tf-idf vectors:\n",
    "tfidf_converter = TfidfVectorizer(stop_words='english')\n",
    "books_tfidf = tfidf_converter.fit_transform([books[name] for name in sorted(books.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Use the cell below to print the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of books: {}\".format(books_tfidf.shape[0]))\n",
    "print(\"Vocabulary size: {}\".format(books_tfidf.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Use the cell below to print a block of the tf-idf matrix (e.g., rows from `0:4` and columns `0:1000`). What do all these number indicate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(books_tfidf[0:4,0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries\n",
    "\n",
    "Now let's prepare a query and convert it to tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A query (feel free to type your own text!)\n",
    "query=['''how to teach education\n",
    "          teacher educational'''.lower().translate(string.punctuation)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now convert the query into its tf-idf representation using `transform` (important: we now have to use `transform` instead of `fit_transform`. The latter defines the vocabulary and the transformation while the former simply applies an exising transformation).\n",
    "\n",
    "**[Task]** Create the variable `query_tfidf` with the tf-idf representation of the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Transform the query to tf-idf representation\n",
    "query_tfidf = tfidf_converter.transform(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the cosine distance between the query and the vector representation of all books. Then we sort the results by distance and print the top 10 retrieved books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute cosine distance between the query and the books\n",
    "cosine_distances = sklearn.metrics.pairwise.cosine_distances(books_tfidf, query_tfidf)\n",
    "# Flatten the distances into an array\n",
    "cosine_distances = cosine_distances.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Create the variable `cosine_distances_sorted_idx` with the indices needed to sort the cosine distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sort cosine distances:\n",
    "cosine_distances_sorted_idx = np.argsort(cosine_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we print the first 10 retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the first 10 retrieved books\n",
    "for i in range(10):\n",
    "    print('{:02d}.\\t File \"{:s}\" \\t(Distance = {:.2f})'.format(i+1, book_names[cosine_distances_sorted_idx[i]],\n",
    "                                                               cosine_distances[cosine_distances_sorted_idx[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Open the two first retrieved books using a text editor. Check the title of the books.\n",
    "\n",
    "**[Task]** Use the code above to retrieve the Declaration of Independence of the United States.\n",
    "\n",
    "**[Solution]** You may use the following query:\n",
    "\n",
    "```python\n",
    "query=['''declaration independence united states\n",
    "          america justice welfare liberty'''.lower().translate(string.punctuation)]\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
