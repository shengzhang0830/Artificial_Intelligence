{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Document Classification\n",
    "\n",
    "*This lab session is inspired by the online [scikit-learn tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html).*\n",
    "\n",
    "In this session, we will apply a simple naive Bayes classifier for document classification. In particular, we will use the [*Twenty Newsgroups* dataset](http://qwone.com/~jason/20Newsgroups), which is a collection of 20,000 documents, partitioned (nearly) evenly across 20 different newsgroups, such as `soc.religion.christian`, `comp.graphics`, etc.  Some of the newsgroups are very closely related to each other (e.g., `comp.sys.ibm.pc.hardware` and `comp.sys.mac.hardware`), while others are highly unrelated (e.g., `misc.forsale` and `soc.religion.christian`).\n",
    "\n",
    "## Data: 20 Newsgroups\n",
    "\n",
    "The data can be downloaded using scikit-learn with\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data_train = fetch_20newsgroups(subset='train',      # Which set to fetch (train/test)\n",
    "                                categories=categories, # Which categories to fetch\n",
    "                                shuffle=True,          # Shuffle the data?\n",
    "                                random_state=42)       # Seed\n",
    "```\n",
    "For the purposes of this session, we will consider only 4 out of the 20 categories, namely,\n",
    "```python\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "```\n",
    "**Download the data.** Thus, we download the training data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This may take some time when executed for the first time\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "data_train = fetch_20newsgroups(subset='train',        # Which set to fetch (train/test)\n",
    "                                categories=categories, # Which categories to fetch\n",
    "                                shuffle=True,          # Shuffle the data?\n",
    "                                random_state=42)       # Seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting familiar with data format.** Now, `data_train` is a dictionary that contains the data. In particular,\n",
    "- `data_train['data']` contains the raw documents.\n",
    "- `data_train['target']` has the label (indicating the category) of each document.\n",
    "- `data_train['target_names']` contains the category names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the number of documents\n",
    "print('Number of documents: {}'.format(len(data_train['data'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the first document\n",
    "print(data_train['data'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the label of the first document\n",
    "print('Label of the first document: {}'.format(data_train['target'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the label names\n",
    "print(data_train['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text preprocessing.** Text preprocessing, tokenizing and filtering of stopwords are included in a high level component of scikit-learn, which is able to build a dictionary of features and transform documents to feature vectors. Let's convert the data into a sparse matrix of word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data_train['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Print the size of `X_train_counts` (use cell below). What do the columns of  correspond to? And the number of rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Print a block of the sparse matrix (use cell below) to verify that it contains integers. For instance, you can print rows `0:2` and columns `0:2000`. What's the meaning of the printed numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer` supports counts of N-grams of words or consecutive characters. In addition, it automatically builds a dictionary of feature indices. For instance, we can find out which index corresponds to the word \"email\" with `count_vect.vocabulary_.get('email')`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The word \"email\" corresponds to token {}'.format(count_vect.vocabulary_.get('email')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Naive Bayes Classifier\n",
    "\n",
    "Here, we use scikit-learn to fit a naive Bayes classifier to our data collection. Scikit-learn includes several variants of this classifier; the most suitable one for document classification is the multinomial (`MultinomialNB`).\n",
    "\n",
    "**Train the classifier.** The `fit` method of `MultinomialNB` learns the parameters of the classifer via maximum a posteriori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_classifier = MultinomialNB().fit(X_train_counts, data_train['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classify documents from the test set.** As an example, here we fetch and classify a document from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fetch the test data\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** In the cell below, print the two first documents in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the outcome of a new document, we need to extract the word counts, as we did before. The only difference is that we call `transform` instead of `fit_transform`, which have already been fit to the training set. The function to make actual predictions is called `predict`. We encapsulate this pipeline on a python function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_pipeline(docs):\n",
    "    X_test_counts = count_vect.transform(docs)\n",
    "    X_test_predicted = naive_classifier.predict(X_test_counts)\n",
    "    return X_test_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** In the cell below, classify the two first documents in the test set and print the predicted labels. For that, use the function `classify_pipeline(data_test['data'][0:2])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Question]** Based on reading the two documents that you printed above, do you think the predicted classes make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation: Accuracy.** We now classify *all* the documents in the test set and compute how many classifications were correct. The percentage of correct classifications is called *accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Predict labels for all documents in test set\n",
    "X_test_predicted = classify_pipeline(data_test['data'])\n",
    "# Compute and print accuracy\n",
    "acc = np.mean(X_test_predicted == data_test['target'])\n",
    "print('The accuracy on the test set is: {:.6f}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation: Precision and recall.** Scikit-learn can also compute more advanced metric, such as precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(data_test['target'], X_test_predicted,\n",
    "                                    target_names=data_test['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Question]** What is the \"support\" column above?\n",
    "\n",
    "**Evaluation: Confusion matrix.** We can also print the confusion matrix using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(metrics.confusion_matrix(data_test['target'], X_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Question]** Take a moment to look at the confusion matrix. What does it indicate? Which are the two categories that are more likely to be confused?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "\n",
    "**[Task]** Now replace the naive Bayes approach with a neural network classifier. For this, you need to use the following scikit-learn function: \n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_classifer = MLPClassifier(solver='adam',      # which variant of gradient descent\n",
    "                             alpha=1.0,                     # regularization term\n",
    "                             activation='relu',             # which non-linear function\n",
    "                             hidden_layer_sizes=(64, 64),   # units per hidden layer\n",
    "                             random_state=42).fit(X_train_counts, data_train['target'])\n",
    "```\n",
    "\n",
    "Fit a neural network and evaluate its performance on the test set (accuracy, precision, recall, F1-score, and confusion matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Task]** Now replace the naive Bayes approach with another more complex classifier. In particular, use a support vector machine (SVM). The SVM is a discriminative classifier that uses a hyperplace to separate the clases, after applying a non-linear transformation of the input. You don't need to know the details now. All you need to know is that scikit-learn also provides a function for the SVM: \n",
    "\n",
    "```python\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train the classifier\n",
    "svm_classifier = LinearSVC(C=1.0).fit(X_train_counts, data_train['target'])\n",
    "```\n",
    "\n",
    "Fit a SVM and evaluate its performance on the test set (accuracy, precision, recall, F1-score, and confusion matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
