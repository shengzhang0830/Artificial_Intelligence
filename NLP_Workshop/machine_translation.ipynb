{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier in class we covered two machine translation models: IBM Model 1 and IBM Model 2.  \n",
    "\n",
    "Given a foreign sentence $f = (f_1,f_2,...,f_{l_f})$ of length $l_f$ and an English sentence  $e = (e_1,e_2,...,e_{l_e})$ of length $l_e$ with an alignment of each English word $e_j$ to a foreign word $f_i$ according to the alignment function $a: j \\rightarrow i$, the translation probability is defined as follows:\n",
    "$$ \\Large p(e,a|f)= \\frac{\\epsilon}{(l_f+1)^{l_e}}\\prod_{j=1}^{l_e}t(e_j|f_{a(j)}) $$\n",
    "\n",
    "Since we include the special NULL token there are actually $l_f+1$ input words.\n",
    "\n",
    "In IBM Model 2 we add an explicit model for alignment. The translation of a foreign input word in position j is modeled by an alignment probability distribution:\n",
    "$$ \\Large a(i|j, l_e, l_f) $$\n",
    "\n",
    "We combine this probability distribution with the lexical translation probability $t(e|f)$ to come up with the final probability:\n",
    "$$ \\Large p(e,a|f)= \\epsilon \\prod_{j=1}^{l_e}t(e_j|f_{a(j)}) a(a(j)|j,l_e,l_f) $$\n",
    "\n",
    "The translation probabilities in both models could be combined with the language model through the noisy-channel model.\n",
    "$$ \\Large argmax_e p(e|f) = argmax_e \\frac{p(f|e)p(e)}{p(f)} =  argmax_e p(f|e)p(e) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab session we are going to utilize IBM Models 1 and 2 to generate lexical translation models for a set of parallel sentences. We will also explore the BLEU score which is the most widely used measure for evaluating machine translation quality. We will also explore the generated alignment from running these two models.  \n",
    "\n",
    "We will be using two datasets that were used as official evaluation collections for the WMT 2009 and 2011 conferences.  \n",
    "\n",
    "The first collection consists of 2000 English-Spanish parallel sentences from the European parliament proceedings.  \n",
    "\n",
    "The second collection consists of 3003 English-German parallel sentences from various news articles. We will be using a subset of these sentence pairs to train an IBM Model 1 and 2.  \n",
    "\n",
    "In addition to this collection we will also be using English sentences generated from various MT systems that took part in the 2011 WMT evaluations. These systems were evaluated on the news test set.  \n",
    "\n",
    "We'll be using the __nltk__ implementation of the two IBM models. More specifically the __nltk.translate__ package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Collections ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both collections are sentence aligned and consist of two files - one for each language. Each line in a file contains a single sentence. Let's first load both collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import nltk.translate\n",
    "stopwords_list = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_words_based_on_fc(all_words):\n",
    "    frequency_count = nltk.FreqDist(all_words)\n",
    "    words =np.array([word for word in frequency_count.keys()])\n",
    "    word_freq=np.array([word for word in frequency_count.values()])\n",
    "    freq_sort = np.argsort(word_freq)[::-1]\n",
    "    word_freq_sort =word_freq[freq_sort]\n",
    "    words_sorted = words[freq_sort]\n",
    "    rank=1\n",
    "    for object in words_sorted:\n",
    "        if (rank<=1000):\n",
    "            print(object+\"\\t\"+str(frequency_count[object]))\n",
    "    rank+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sentences(sent_file):\n",
    "    coll_sent = list()\n",
    "    for sent in sent_file.readlines():\n",
    "        sent_words = sent.split(\" \")\n",
    "        sent_words = [word for word in sent_words]\n",
    "        coll_sent.append(sent_words)\n",
    "    return coll_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_euro_file = open(\"../../../data/mt/europarl/test2008.true.en\",'r')\n",
    "sp_euro_file = open(\"../../../data/mt/europarl/test2008.true.es\",'r')\n",
    "\n",
    "sent_limit = 500\n",
    "\n",
    "en_euro = list()\n",
    "all_en_euro_words = list()\n",
    "r_count = 0\n",
    "for sent in en_euro_file.readlines():\n",
    "    r_count += 1\n",
    "    if (r_count % 100 == 0):\n",
    "        print(r_count)\n",
    "    if (r_count==sent_limit):\n",
    "        break\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    #sent_words_tok = [word for word in sent_words]\n",
    "    sent_words_tok = [word.lower() for word in sent_words if word not in stopwords_list]\n",
    "    en_euro.append(sent_words_tok)\n",
    "    for words in sent_words_tok:\n",
    "        all_en_euro_words.append(words)\n",
    "\n",
    "sp_euro = list()\n",
    "all_sp_euro_words = list()\n",
    "r_count = 0\n",
    "for sent in sp_euro_file.readlines():\n",
    "    r_count += 1\n",
    "    if (r_count % 100 == 0):\n",
    "        print(r_count)\n",
    "    if (r_count==sent_limit):\n",
    "        break\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    #sent_words_tok = [word for word in sent_words]\n",
    "    sent_words_tok = [word.lower() for word in sent_words if word not in stopwords_list]\n",
    "    sp_euro.append(sent_words_tok)\n",
    "    for words in sent_words_tok:\n",
    "        all_sp_euro_words.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_news_file = open(\"../../../data/mt/news/newstest2011-ref.en\",'r')\n",
    "de_news_file = open(\"../../../data/mt/news/newstest2011-ref.de\",'r')\n",
    "\n",
    "sent_limit = 500\n",
    "\n",
    "en_news = list()\n",
    "all_en_news_words = list()\n",
    "r_count = 0\n",
    "for sent in en_news_file.readlines():\n",
    "    r_count += 1\n",
    "    if (r_count % 100 == 0):\n",
    "        print(r_count)\n",
    "    if (r_count==sent_limit):\n",
    "        break\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    #sent_words_tok = [word for word in sent_words]\n",
    "    sent_words_tok = [word.lower() for word in sent_words if word not in stopwords_list]\n",
    "    en_news.append(sent_words_tok)\n",
    "    for words in sent_words_tok:\n",
    "        all_en_news_words.append(words)\n",
    "\n",
    "de_news = list()\n",
    "all_de_news_words = list()\n",
    "r_count = 0\n",
    "for sent in de_news_file.readlines():\n",
    "    r_count += 1\n",
    "    if (r_count % 100 == 0):\n",
    "        print(r_count)\n",
    "    if (r_count==sent_limit):\n",
    "        break\n",
    "    sent_words = nltk.word_tokenize(sent)\n",
    "    #sent_words_tok = [word for word in sent_words]\n",
    "    sent_words_tok = [word.lower() for word in sent_words if word not in stopwords_list]\n",
    "    de_news.append(sent_words_tok)\n",
    "    for words in sent_words_tok:\n",
    "        all_de_news_words.append(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both parallel collections loaded into separate lists. Let's first build a MT model. We'll start with the IBM Model 1. In order to train this model we would first need to store the parallel sentences into an alignment object. But before we proceed let's first get a sense of the type of words that are in both collections and both languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_words_based_on_fc(all_en_euro_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_words_based_on_fc(all_sp_euro_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_words_based_on_fc(all_en_news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_words_based_on_fc(all_de_news_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now store these two parallel collections into alignment objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "europarl_corpus = list()\n",
    "for en, sp in zip(en_euro, sp_euro):\n",
    "    europarl_corpus.append(nltk.translate.AlignedSent(en,sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_corpus = list()\n",
    "for en, de in zip(en_news, de_news):\n",
    "    news_corpus.append(nltk.translate.AlignedSent(en,de))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Model 1 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the IBM Model 1 using the Europarl bitext. We do this with the __nltk.translate.IBMModel1__ method. Both IBM models use the EM algorithm to train the lexical translation models. Depending on the number of sentences in the collection running the EM algorithm may take some time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm1_europarl = nltk.translate.IBMModel1(europarl_corpus, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 1]**  \n",
    "The __.translation_table__ method allows us to obtain the estimated translation probabilities of two words. For example, the code below will give us the translation probability of the English word \"together\" to the Spanish word \"todos\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm1_europarl.translation_table[\"together\"][\"todos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this method to explore the trained translatio model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 2]**  \n",
    "Train the IBM Model1 using the news collection and explore the translation probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 2]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Model 2 ##\n",
    "So far we used IBM Model 1. In this part of the lab session we'll be using IBM Model 2. This model is implemented under the implemented under the __nltk.translate.IBMModel2__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ibm2_europarl = nltk.translate.IBMModel2(europarl_corpus, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 3]**  \n",
    "Let's use IBM Model 2 to repeat the above assignments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 3]**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing BLEU Score ##\n",
    "In this part of the lab session we are going to analyze the performance of various MT systems using the BLEU score. The nltk.translate module implements this measure through the __nltk.translate.bleu_score__ method. Provided in the mt folder are the translation outputs of various MT systems that participated in the WMT 2011 evaluations. Let's load the generated sentences of one such MT system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "ref_trans_file = open(\"../../../data/mt/news/newstest2011-ref.de\",'r')\n",
    "\n",
    "mt1_file = open(\"../../../data/mt/news/newstest2011.en-de.jhu\",'r')\n",
    "\n",
    "mt1_output = load_sentences(mt1_file)\n",
    "ref_trans = load_sentences(ref_trans_file)\n",
    "\n",
    "bleu_mt1 = nltk.translate.bleu_score.corpus_bleu(ref_trans,mt1_output,smoothing_function=SmoothingFunction().method4)\n",
    "print(bleu_mt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 4]**  \n",
    "Compare the BLEU score performance of this system with other MT system that participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 4]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Assignment 5 (Optional)]**\n",
    "So far we've only the top 500 sentences from each bitext to train our models. Re-train the two models using a larger collection ($\\geq 500$) of sentences and observe the translation probabilities of these models. __Note__: Training with a larger number of sentences may take longer period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Solution 5 (Optional)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
