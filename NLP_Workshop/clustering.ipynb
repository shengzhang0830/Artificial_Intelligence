{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with scikit-learn\n",
    "\n",
    "Clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups. It is a central tool of exploratory data mining, statistical data analysis, and machine learning. Clustering is a form of unsupervised learning, in that the datapoints are grouped without information about labels.\n",
    "\n",
    "Cluster analysis itself is not one specific algorithm, but the name of a general task. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to find them. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.\n",
    "\n",
    "We will cover here how to implement two simple clustering methods using scikit-learn:\n",
    "- k-means\n",
    "- Mixture of Gaussians\n",
    "\n",
    "For the full documentation, we encourage you to have a look at the [official scikit-learn documentation on clustering](http://scikit-learn.org/stable/modules/clustering.html).\n",
    "\n",
    "## Generate toy data\n",
    "\n",
    "For the purposes of this tutorial, we will generate a toy dataset of $N=1000$ datapoints and $D=2$ dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "# Generate toy data\n",
    "N = 1000\n",
    "D = 2\n",
    "X = np.zeros((N, D))\n",
    "means_k = [[1, 1], [-1, 1], [1, -1], [-1, -1]]\n",
    "for k in range(4):\n",
    "    idx_k = np.array(list(range(N//4)))\n",
    "    idx_k += k*(N//4)\n",
    "    X[idx_k, :] = means_k[k] + 0.5*np.random.randn(N//4, D)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â k-means\n",
    "\n",
    "The k-means algorithm clusters data by trying to separate samples in $K$ groups of equal variance, minimizing a criterion known as the within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.\n",
    "\n",
    "The k-means algorithm divides a set of $N$ samples into $K$ disjoint clusters, each described by the mean $\\mu_k$ of the samples in the cluster. The means are commonly called the cluster *centroids*; note that they are not data points, although they live in the same space. The k-means algorithm aims to choose centroids that minimise the within-cluster sum-of-squares criterion, i.e.,\n",
    "$$\n",
    "\\min \\sum_{n=1}^{N} ||x_n-\\mathbf{\\mu}_{z_n}||^2,\n",
    "$$\n",
    "being $z_n\\in\\{1,\\ldots,K\\}$ the indicator variable of the cluster assigned to datapoint $n$.\n",
    "\n",
    "**Example of usage.** We use `sklearn.cluster.KMeans`. See [this page](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) for the full documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans_model = KMeans(n_clusters=4).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtaining the labels.** `kmeans_model.labels_` gives us the labels for all datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels = kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information, e.g., to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the clustered data (using a different color per cluster)\n",
    "plt.figure()\n",
    "for k in range(4):\n",
    "    idx_k = (all_labels==k)\n",
    "    plt.scatter(X[idx_k,0], X[idx_k,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions on unseen data.** We can obtain the labels of new datapoints as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xnew = np.random.randn(10, D)\n",
    "labels_new = kmeans_model.predict(Xnew)\n",
    "print(labels_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain the cluster means.** The cluster means can be obtained as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_means = kmeans_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the clustered data (using a different color per cluster)\n",
    "plt.figure()\n",
    "for k in range(4):\n",
    "    idx_k = (all_labels==k)\n",
    "    plt.scatter(X[idx_k,0], X[idx_k,1])\n",
    "plt.scatter(cluster_means[:,0], cluster_means[:,1], marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations of k-means.** For a list of the limitations of this algorithm, we refer the reader to [this page](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py).\n",
    "\n",
    "## Mixture of Gaussians\n",
    "\n",
    "Gaussian mixture models are a type of probabilistic model. For each cluster, they assume that the data in that cluster has been generated from a Gaussian distribution of certain mean and covariance. The goal is thus to find the parameters (means and covariances) of these Gaussian distributions.\n",
    "\n",
    "The likelihood can be formally described as follows:\n",
    "$$\n",
    "p(x_n|z_n=k) = \\frac{1}{2\\pi|\\Sigma_k|^{1/2}} \\exp\\left\\{- \\frac{1}{2} (x_n-\\mu_k)^\\top \\Sigma_k^{-1}(x_n-\\mu_k) \\right\\},\n",
    "$$\n",
    "where $z_n$ is a cluster indicator variable; $\\mu_k$ is the cluster mean, and $\\Sigma_k$ is the cluster covariance.\n",
    "\n",
    "The joint probability $p(x_n,z_n)$ has an additional parameter: the *weight* of each cluster. Formally,\n",
    "$$\n",
    "p(x_n,z_n=k) = p(x_n|z_n=k)p(z_n=k)= w_k p(x_n|z_n=k),\n",
    "$$\n",
    "where $w_k$ is the prior probability of cluster $k$ (this allows modeling data clustered in uneven groups).\n",
    "\n",
    "These equations, together with the corresponding priors over $w_k$, $\\mu_k$, and $\\Sigma_k$, form the model specification.\n",
    "\n",
    "**Example of usage.** The package `sklearn.mixture` allows us to implement a Gaussian mixture, which is fit to the data via the expectation-maximization (EM) algorithm. See the full documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture). Its usage is similar to k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gmm_model = GaussianMixture(n_components=4).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtaining the labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels = gmm_model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the clustered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the clustered data (using a different color per cluster)\n",
    "plt.figure()\n",
    "for k in range(4):\n",
    "    idx_k = (all_labels==k)\n",
    "    plt.scatter(X[idx_k,0], X[idx_k,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions on unseen data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xnew = np.random.randn(10, D)\n",
    "labels_new = gmm_model.predict(Xnew)\n",
    "print(labels_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain the cluster parameters.** We can obtain the cluster means, covariances, and weights, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmm_means = gmm_model.means_\n",
    "gmm_cov = gmm_model.covariances_\n",
    "gmm_weight = gmm_model.weights_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can, e.g., plot the means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the clustered data (using a different color per cluster)\n",
    "plt.figure()\n",
    "for k in range(4):\n",
    "    idx_k = (all_labels==k)\n",
    "    plt.scatter(X[idx_k,0], X[idx_k,1])\n",
    "plt.scatter(gmm_means[:,0], gmm_means[:,1], marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the covariance matrices are approximately diagonal, and that the weights are approximately uniform. This is a consequence of how we generated the data (with zero covariance between both dimensions, and evenly distributed across groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gmm_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gmm_weight)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
